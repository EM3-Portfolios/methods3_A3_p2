---
title: "Assignment 3 - Part 2 - Diagnosing Schizophrenia from Voice"
author: "Riccardo Fusaroli"
date: "October 17, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Assignment 3 - Part 2 - Diagnosing schizophrenia from voice

In the previous part of the assignment you generated a bunch of "features", that is, of quantitative descriptors of voice in schizophrenia. We then looked at whether we could replicate results from the previous literature.
We now want to know whether we can automatically diagnose schizophrenia from voice only, that is, relying on the set of features you produced last time, we will try to produce an automated classifier.
Again, remember that the dataset containst 7 studies and 3 languages. Feel free to only include Danish (Study 1-4) if you feel that adds too much complexity.

Issues to be discussed your report:
- Should you run the analysis on all languages/studies at the same time? 
- Choose your best acoustic feature from part 1. How well can you diagnose schizophrenia just using it?
- Identify the best combination of acoustic features to diagnose schizophrenia using logistic regression.
- Discuss the "classification" process: which methods are you using? Which confounds should you be aware of? What are the strength and limitation of the analysis?
- Bonus question: Logistic regression is only one of many classification algorithms. Try using others and compare performance. Some examples: Discriminant Function, Random Forest, Support Vector Machine, etc. The package caret provides them. 
- Bonus Bonus question: It is possible combine the output of multiple  classification models to improve classification accuracy. For inspiration see,
https://machinelearningmastery.com/machine-learning-ensembles-with-r/
 The interested reader might also want to look up 'The BigChaos Solution to the Netflix Grand Prize'

## Learning objectives
- Learn the basics of classification in a machine learning framework
- Design, fit and report logistic regressions
- Apply feature selection techniques

### Let's start

We first want to build a logistic regression to see whether you can diagnose schizophrenia from your best acoustic feature. Let's use the full dataset and calculate the different performance measures (accuracy, sensitivity, specificity, PPV, NPV, ROC curve). You need to think carefully as to how we should (or not) use study and subject ID.

Then cross-validate the logistic regression and re-calculate performance on the testing folds. N.B. The cross-validation functions you already have should be tweaked: you need to calculate these new performance measures. Alternatively, the groupdata2 and cvms package created by Ludvig are an easy solution. 

N.B. the predict() function generates log odds (the full scale between minus and plus infinity). Log odds > 0 indicates a choice of 1, below a choice of 0.
N.N.B. you need to decide whether calculate performance on each single test fold or save all the prediction for test folds in one datase, so to calculate overall performance.
N.N.N.B. Now you have two levels of structure: subject and study. Should this impact your cross-validation?
N.N.N.N.B. A more advanced solution could rely on the tidymodels set of packages (warning: Time-consuming to learn as the documentation is sparse, but totally worth it)



```{r loading data and creating logreg model + lots of trial and error with graphs and outliers}
library(lme4)
library(tidyverse)
library(caret)
library(pROC)

# loading dataframe from last time
df <- read.csv("df_before_models-kopi.csv")

# let's use pause duration as out best aucustic feature, because that one has the highest effects size estimate from the last part. 
# It's also the feature that had the highest Hedge's G in the meta analysis. 

# making sure Diagnosis as a factor to use for the logistic regression
df$Diagnosis <- as.factor(df$Diagnosis)

# making our first model
just_pausedur_model <- glmer(Diagnosis ~ pause_duration + (1 | ID), data = df, family = "binomial")

# we use subject ID as random intercept because of repeated measures. 
# We do not use study (because we are only using the Danish data, and we previously concluded that the Danish studies are not different)

summary(just_pausedur_model)
# wait... pause_duration is not even significant... le'ts test with the other features...
just_variability_model <- glmer(Diagnosis ~ sd_f0 + (1 | ID), data = df, family = "binomial")
just_proportion_model <- glmer(Diagnosis ~ proportion_of_spoken_time + (1 | ID), data = df, family = "binomial")
just_speechrate_model <- glmer(Diagnosis ~ speechrate..nsyll.dur. + (1 | ID), data = df, family = "binomial")

summary(just_variability_model)
summary(just_proportion_model)
summary(just_speechrate_model) # this is the only one that has ANY *** on fixed effect !



# trying things:
ggplot(df, aes(pause_duration, scz)) + 
  geom_jitter(height = 0.05) +
  geom_smooth(method = "glm", method.args = list(family = "binomial"))

ggplot(df, aes(sd_f0, scz)) +     # sd_f0 scores are copied for each ID. multiple of the same values cause the stripes in the graph.
  geom_jitter(height = 0.05) +
  geom_smooth(method = "glm", method.args = list(family = "binomial"))

ggplot(df, aes(proportion_of_spoken_time, scz)) + 
  geom_jitter(height = 0.05) +
  geom_smooth(method = "glm", method.args = list(family = "binomial"))

ggplot(df, aes(speechrate..nsyll.dur., scz)) + 
  geom_jitter(height = 0.05) +
  geom_smooth(method = "glm", method.args = list(family = "binomial"))


# I will now try to remove the outlier from pause_duration
#Remove outliers
df_no_out<-subset(df,df$pause_duration<mean(df$pause_duration)+3*sd(df$pause_duration)) 
#removing cases where pause_duration is more than 3 sd away from the mean - common proceduce given definition of normal dis.

ggplot(df_no_out, aes(pause_duration, scz, color = ID)) + 
  geom_jitter(height = 0.05) +
  geom_smooth(method = "glm", method.args = list(family = "binomial"))
# wait... if we only have one data point for each ID, then why do we have ID ans random intercept? do we only have 1 obs per ID?
# ok speechrate..nsyll.dur. is different, pause_duration is different, proportion is different
# sd_f0 is the same for one ID. (so for this one it woudn't make sense to have random ID? is that what that means? or only 1 per ID at least)



# now i need to make a new model without the outlier and use summary to see if there is significance now.
no_out_pausedur_model <- glmer(Diagnosis ~ pause_duration + (1 | ID), data = df_no_out, family = "binomial")
summary(no_out_pausedur_model) # still no

ggplot(df, aes(ID, Diagnosis))+
  geom_point()
```


```{r calculating performance measures - accuracy, sensitivity, specificity, PPV, NPV, ROC curve}
# If we have to recalculate all perfomrance measures on the folds in the next task (cross-validation) would't it be a good idea to create a function that calculates and saves all 6 measures?

# look to the lecture to see how measures are found
# For this code right now I am using the speechrate..nsyll.dur. model because it seems to not be problematic...


# first we need to define this function: https://sebastiansauer.github.io/convert_logit2prob/
logit2prob <- function(logit){
  odds <- exp(logit)
  prob <- odds / (1 + odds)
  return(prob)
}

# we have to make R stop using numbers like 0.0004835312 instead of 4.835312e-04 (scientific notation) because otherwise the "Control" fucks up
options(scipen = 999)

# then here is the confusion matrix code, taken from the powerpoint of this week's lecture
df$pred_speechrate <- logit2prob(predict(just_speechrate_model))
df$pred_speechrate[df$pred_speechrate >0.5] = "Schizophrenia"
df$pred_speechrate[df$pred_speechrate <=0.5] = "Control" # something is wrong here - i found out why! it's because of scientific notation!
df$pred_speechrate <-  as.factor(df$pred_speechrate)
df$Diagnosis <- as.factor(df$Diagnosis)
confusionMatrix(data = df$pred_speechrate, reference = df$Diagnosis, positive = "Schizophrenia")

# OK, accuracy, sensitivity, specificity, PPV and NPV are all 1. Very sus
# Let's look at ROC 

rocCurve_speechrate <- roc(response = df$Diagnosis, predictor = df$pred_speechrate)
auc(rocCurve_speechrate)# this is the ROC curve value we are looking for :-)
ci(rocCurve_speechrate)
plot(rocCurve_speechrate, legacy.axes = TRUE) 

```


```{r making a function to calculate performance measures}
roc_and_confmat <- function(model, pred_predictor_name){
  df$pred_predictor_name <- logit2prob(predict(model))

  return(predictor_name)
}

roc_and_confmat(just_speechrate_model, pred_speechrate)
```


```{r cross-validation}
# The following link explains choice of number of folds:
# https://datascience.stackexchange.com/questions/28158/how-to-calculate-the-fold-number-k-fold-in-cross-validation

# a problem you may run into when validating: balanced and unbalanced data
# meaning unbalanced folds. The folds have to be balanced, so how do we tell them to do that?
pacman::p_load(groupdata2)

# we want to add a column to the df that gives a number to different parts of the dataset (folds)
df$ID <- as.factor(df$ID)
df <- fold(df, k = 5, cat_col = "Diagnosis", id_col = "ID") %>% arrange(.folds)
  #cat_col means categorizing column 
  # also if we have repeated measures we don't want to split one participant over different fold

ggplot(df, aes(.folds, Diagnosis))+
  geom_jitter()
```

